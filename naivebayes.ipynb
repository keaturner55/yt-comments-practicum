{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Trained on the Sentiment 140 data set\n",
    "In this section we'll attempt to build a custom Naive Bayes model for text classification. Naive Bayes classifiers use conditional probabilities based on pre-tagged data, so weWill use pre-tagged data set containing about 1.6 million tweets tagged for negative, positive, and neutral sentiment. Try training and running a multinomial Naive Bayes model from sklearn. To \"vectorize\" the text, we must first apply a TFIDF (term-frequency/inverse document frequency) transformation to model the text values as numeric values.\n",
    "\n",
    "https://www.kaggle.com/datasets/kazanova/sentiment140?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from comment_scraper import get_sql_table\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn-notebook')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer # tweet tokenizer is good for handling common words/symbols/emoticons found in social media data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'C:\\\\Users\\\\keatu\\\\Regis_archive\\\\practicum2_data\\\\'\n",
    "dbname = os.path.join(DATA_DIR, \"Youtube_Data_msnbc.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"]\n",
    "s140_test = pd.read_csv(os.path.join(DATA_DIR,\"resources\",\"s140_test.csv\"), names = colnames)\n",
    "s140_train = pd.read_csv(os.path.join(DATA_DIR,\"resources\",\"s140_train.csv\"), names = colnames, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lookt at distribution\n",
    "s140_train['target'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data pre-processing function that will be applied to each text string\n",
    "def clean_data(text):\n",
    "    #stopwords = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = TweetTokenizer()\n",
    "    text_vector = []\n",
    "    for each_text in text:\n",
    "        lemmatized_tokens = []\n",
    "        tokens=tokenizer.tokenize(each_text.lower())\n",
    "        pos_tags=pos_tag(tokens)\n",
    "        for each_token, tag in pos_tags: \n",
    "            if tag.startswith('NN'): \n",
    "                pos='n'\n",
    "            elif tag.startswith('VB'): \n",
    "                pos='v'\n",
    "            else: \n",
    "                pos='a'\n",
    "            lemmatized_token=lemmatizer.lemmatize(each_token, pos)\n",
    "            lemmatized_tokens.append(lemmatized_token)\n",
    "        text_vector.append(' '.join(lemmatized_tokens))\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', '', 'l', 'o', 'v', 'e', '', 'm', 'o', 'v', 'i', 'e', 's']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(\"I love movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators=[('cleaner', FunctionTransformer(clean_data)), \n",
    "            ('vectorizer', TfidfVectorizer(max_features=100000, ngram_range=(1, 2)))]\n",
    "preprocessing_pipeline=Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TfidfVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class TfidfVectorizer(CountVectorizer)\n",
      " |  TfidfVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |  \n",
      " |  Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      " |  \n",
      " |  Equivalent to :class:`CountVectorizer` followed by\n",
      " |  :class:`TfidfTransformer`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'} (default='strict')\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None} (default=None)\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |  \n",
      " |  lowercase : boolean (default=True)\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  preprocessor : callable or None (default=None)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default=None)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |      .. versionchanged:: 0.21\n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default=None)\n",
      " |      If a string, it is passed to _check_stop_list and the appropriate stop\n",
      " |      list is returned. 'english' is currently the only supported string\n",
      " |      value.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n) (default=(1, 1))\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int (default=1.0)\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int (default=1)\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None (default=None)\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional (default=None)\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents.\n",
      " |  \n",
      " |  binary : boolean (default=False)\n",
      " |      If True, all non-zero term counts are set to 1. This does not mean\n",
      " |      outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      " |      is binary. (Set idf and normalization to False to get 0/1 outputs.)\n",
      " |  \n",
      " |  dtype : type, optional (default=float64)\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  norm : 'l1', 'l2' or None, optional (default='l2')\n",
      " |      Each output row will have unit norm, either:\n",
      " |      * 'l2': Sum of squares of vector elements is 1. The cosine\n",
      " |      similarity between two vectors is their dot product when l2 norm has\n",
      " |      been applied.\n",
      " |      * 'l1': Sum of absolute values of vector elements is 1.\n",
      " |      See :func:`preprocessing.normalize`\n",
      " |  \n",
      " |  use_idf : boolean (default=True)\n",
      " |      Enable inverse-document-frequency reweighting.\n",
      " |  \n",
      " |  smooth_idf : boolean (default=True)\n",
      " |      Smooth idf weights by adding one to document frequencies, as if an\n",
      " |      extra document was seen containing every term in the collection\n",
      " |      exactly once. Prevents zero divisions.\n",
      " |  \n",
      " |  sublinear_tf : boolean (default=False)\n",
      " |      Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  idf_ : array, shape (n_features)\n",
      " |      The inverse document frequency (IDF) vector; only defined\n",
      " |      if ``use_idf`` is True.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = TfidfVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> print(vectorizer.get_feature_names())\n",
      " |  ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      " |  >>> print(X.shape)\n",
      " |  (4, 9)\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      " |  \n",
      " |  TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      " |      matrix of counts.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TfidfVectorizer\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf from training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : TfidfVectorizer\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn vocabulary and idf, return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  transform(self, raw_documents, copy=True)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Uses the vocabulary and document frequencies (df) learned by fit (or\n",
      " |      fit_transform).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          an iterable which yields either str, unicode or file objects\n",
      " |      \n",
      " |      copy : boolean, default True\n",
      " |          Whether to copy X and operate on the copy or perform in-place\n",
      " |          operations.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Tf-idf-weighted document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  idf_\n",
      " |  \n",
      " |  norm\n",
      " |  \n",
      " |  smooth_idf\n",
      " |  \n",
      " |  sublinear_tf\n",
      " |  \n",
      " |  use_idf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CountVectorizer:\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : string\n",
      " |          The string to decode\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=s140_train['text']\n",
    "y=s140_train['target']\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188061     I hate the internet tonight. It ruined everyth...\n",
       "992770     @pandamerv yeah! I'm listening to it right now...\n",
       "834725     @Shinybiscuit lemme know when tweaked, and I'l...\n",
       "541887     I'm upset  I can't find @ddlovato and @selenag...\n",
       "1127231    In indiana at the pinks all out show!  these c...\n",
       "                                 ...                        \n",
       "979613     @iamxc You can also play Tongits on your PC or...\n",
       "1413945                @Flyestoncampus come to my job dweeb \n",
       "539939                    the weather is killing me too hot \n",
       "1527359    wahahah playing sims 3 online but theres less ...\n",
       "1058393    your adv banner/page for FREE in my magazine! ...\n",
       "Name: text, Length: 1200000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:179: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(joblib_version) < '0.12':\n",
      "c:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'I hate the internet tonight. It ruined everything '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a9eb60611717>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fit and transform the pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_transformed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocessing_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \"\"\"\n\u001b[0;32m    386\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    389\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         if (self.check_inverse and not (self.func is None or\n\u001b[0;32m    134\u001b[0m                                         self.inverse_func is None)):\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\u001b[0m in \u001b[0;36m_check_input\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"M8[ns]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;31m# ----------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\keatu\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'I hate the internet tonight. It ruined everything '"
     ]
    }
   ],
   "source": [
    "# Fit and transform the pipeline\n",
    "X_train_transformed=preprocessing_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3cb9dc819795652304a44ab58ca5ddcf1f35aa144f974701a2d129f107a2fd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
